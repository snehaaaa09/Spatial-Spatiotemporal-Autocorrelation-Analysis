{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# User-defined functions\n",
    "# All data preparation functions for GDP data and explanatory variables\n",
    "from Functions.data_prep import data_GDP, data_explanatory\n",
    "# Imports for Moran's I analysis and plotting graphs\n",
    "from Functions.spatial_functions import spatial_weight_matrix, neighborhood_dict_creation, local_moran_density_plot, local_moran_plots_STAC, lisa_cluster_map_STAC, lisa_update_STAC, quadrant_plot_STAC\n",
    "# Spatiotemporal outlier analysis\n",
    "from Functions.spatiotemporal_autocorrelation import year_df_creation, even_time_period_GDP, global_STAC_val, local_STAC_per_region, local_STAC, region_df_creation, corr_time_series, deviation_time_series, global_STAC\n",
    "\n",
    "# All required imports\n",
    "from esda.moran import Moran_Local\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "random.seed(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Data preparation and cleaning for GDP data\n",
    "# Setting time_period option to all years since this is spatiotemporal analysis and all time periods needed\n",
    "time_period_option = 'all_years'\n",
    "NUTS_level = 2\n",
    "gdf_lvl = data_GDP(NUTS_level, time_period_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Making sure that each region has the same number of time periods which is required for STAC Analysis!\n",
    "gdf_lvl = even_time_period_GDP(gdf_lvl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Local STAC calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Extract only for one year to calculate spatial weight (preventing repeated regions)!!\n",
    "region_df = year_df_creation(gdf_lvl, 2004)\n",
    "region_df = region_df.drop(columns=['TIME_PERIOD'])\n",
    "## Making sure that the index of region is NUTS_ID\n",
    "region_df.set_index('NUTS_ID', inplace=True)\n",
    "\n",
    "# Spatial Weights Creation\n",
    "w_adaptive = spatial_weight_matrix(region_df, 15)\n",
    "neighbor_weights_dict = neighborhood_dict_creation(w_adaptive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "local_STAC_dict = local_STAC(neighbor_weights_dict, gdf_lvl, region_df, 'GDP_VALUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remember to reset index before updating it with lisa values and corresponding significance and quadrant values\n",
    "region_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Updating the dataframe to include local LISA information\n",
    "variable = \"GDP_VALUE\"\n",
    "lisa_results = lisa_update_STAC(region_df, gdf_lvl, local_STAC_dict, w_adaptive, variable)\n",
    "# Extracting the results\n",
    "region_df = lisa_results[0]\n",
    "quadrant_df = lisa_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_df = gdf_lvl.sort_values(by=['NUTS_ID', 'TIME_PERIOD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Global STAC Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "835152411781.0594"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable = 'GDP_VALUE'\n",
    "denominator = 0.0\n",
    "# Group by TIME_PERIOD and calculate the mean GDP value summed over all regions for each year\n",
    "mean_gdp_sum_per_year = final_df.groupby('TIME_PERIOD')[variable].mean()\n",
    "# Convert the resulting series to a DataFrame\n",
    "mean_gdp_df = mean_gdp_sum_per_year.to_frame(name='Mean_variable')\n",
    "mean_gdp_array = mean_gdp_df['Mean_variable']\n",
    "# Calculate denominator (same for each region)\n",
    "for region, neighbor_weights in neighbor_weights_dict.items():\n",
    "    time_series_i = region_df_creation(final_df, region)\n",
    "    corr_i = corr_time_series(time_series_i[variable], mean_gdp_array)\n",
    "    dev_i = deviation_time_series(corr_i, time_series_i[variable], mean_gdp_array)\n",
    "    dev_i_squared = dev_i ** 2\n",
    "    denominator += dev_i_squared\n",
    "denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing variables\n",
    "spatial_w_sum = 0.0\n",
    "# Calculate sum of all spatial weights\n",
    "# Iterate over all regions\n",
    "for region, neighbor_weights in neighbor_weights_dict.items():\n",
    "    # Iterate over all neighbor weights for the current region\n",
    "    for weight in neighbor_weights.values():\n",
    "        # Add the weight to the total sum\n",
    "        spatial_w_sum += weight\n",
    "# Calculate total number of regions\n",
    "N = len(region_df)\n",
    "first_term = N / spatial_w_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 999 simulations...\n",
      "All simulations completed in 1530.54 seconds\n"
     ]
    }
   ],
   "source": [
    "def global_randomization(df, num_simulations=999, num_jobs=-1):\n",
    "    regions = df['NUTS_ID'].unique()\n",
    "\n",
    "    def run_simulation(i):\n",
    "        # Create a randomized dataset by shuffling the entire time series across regions\n",
    "        shuffled_df = df.copy()\n",
    "        shuffled_regions = np.random.permutation(regions)\n",
    "        print(shuffled_regions)\n",
    "        # Mapping of original regions to shuffled regions\n",
    "        region_mapping = {original: shuffled for original, shuffled in zip(regions, shuffled_regions)}\n",
    "        # Apply the new mapping to the dataframe\n",
    "        shuffled_df['NUTS_ID'] = shuffled_df['NUTS_ID'].map(region_mapping)\n",
    "        shuffled_df = shuffled_df.sort_values(by=['NUTS_ID', 'TIME_PERIOD']).reset_index(drop=True)\n",
    "        # Calculate global LISA values for the shuffled dataset\n",
    "        global_STAC = global_STAC_val(neighbor_weights_dict, shuffled_df, variable, mean_gdp_array, first_term)\n",
    "\n",
    "        return global_STAC\n",
    "\n",
    "    print(f\"Starting {num_simulations} simulations...\")\n",
    "    start_time = time.time()\n",
    "    results = Parallel(n_jobs=num_jobs)(delayed(run_simulation)(i) for i in range(num_simulations))\n",
    "    end_time = time.time()\n",
    "    print(f\"All simulations completed in {end_time - start_time:.2f} seconds\")\n",
    "    return np.array(results)\n",
    "\n",
    "# Running with print statements to monitor progress\n",
    "simulated_global_STACs = global_randomization(gdf_lvl, num_simulations=999, num_jobs=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Global STAC p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.6207243325948555"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate global STAC value\n",
    "original_global_STAC = global_STAC(neighbor_weights_dict, gdf_lvl, region_df, 'GDP_VALUE')\n",
    "original_global_STAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Global LISA: 0.6207243325948555\n",
      "P-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the p-value\n",
    "p_value = np.mean(simulated_global_STACs >= original_global_STAC)\n",
    "\n",
    "print(f\"Original Global LISA: {original_global_STAC}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LISA Permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "WARNING: The code below takes 12 hours to run due to the high number of permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 999 simulations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20211147\\PycharmProjects\\DataMiningCodes\\venv\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def conditional_randomization(df, num_simulations=999, num_jobs=-1):\n",
    "    regions = df['NUTS_ID'].unique()\n",
    "    n = len(regions)\n",
    "    region_index_map = {region: idx for idx, region in enumerate(regions)}\n",
    "    # Precompute fixed series for each region to avoid repeated filtering\n",
    "    fixed_series_dict = {region: df[df['NUTS_ID'] == region] for region in regions}\n",
    "    # Precompute remaining series for each region\n",
    "    remaining_series_dict = {region: df[df['NUTS_ID'] != region] for region in regions}\n",
    "    def run_simulation(i):\n",
    "        simulated_I = np.zeros(n)\n",
    "        for region in regions:\n",
    "            fixed_series = fixed_series_dict[region]\n",
    "            remaining_regions = remaining_series_dict[region]\n",
    "            shuffled_regions = np.random.permutation(remaining_regions['NUTS_ID'].unique())\n",
    "            # List to collect DataFrames and concatenate once\n",
    "            df_list = [fixed_series.copy()]\n",
    "            for original_region, shuffled_region in zip(regions[regions != region], shuffled_regions):\n",
    "                shuffled_series = remaining_regions[remaining_regions['NUTS_ID'] == shuffled_region].copy()\n",
    "                shuffled_series['NUTS_ID'] = original_region\n",
    "                df_list.append(shuffled_series)\n",
    "            randomized_df = pd.concat(df_list, ignore_index=True)\n",
    "            randomized_df = randomized_df.sort_values(by=['NUTS_ID', 'TIME_PERIOD']).reset_index(drop=True)\n",
    "            simulated_I[region_index_map[region]] = local_STAC_per_region(neighbor_weights_dict, randomized_df, region_df, variable, region, denominator, mean_gdp_array)\n",
    "        return simulated_I\n",
    "\n",
    "    print(f\"Starting {num_simulations} simulations...\")\n",
    "    start_time = time.time()\n",
    "    results = Parallel(n_jobs=num_jobs)(delayed(run_simulation)(i) for i in range(num_simulations))\n",
    "    end_time = time.time()\n",
    "    print(f\"All simulations completed in {end_time - start_time:.2f} seconds\")\n",
    "    return np.array(results)\n",
    "\n",
    "# Assuming final_df, neighbor_weights_dict, and region_df are defined\n",
    "# Running with print statements to monitor progress\n",
    "simulated_LISAs = conditional_randomization(final_df, num_simulations=999, num_jobs=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Saving LISA Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# You can optionally provide column names, here we'll use generic names\n",
    "column_names = [f'Column{i+1}' for i in range(simulated_LISAs.shape[1])]\n",
    "df = pd.DataFrame(simulated_LISAs, columns=column_names)\n",
    "\n",
    "# Step 4: Save the DataFrame to a CSV file\n",
    "df.to_csv('results/simulated_LISAs_STAC_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### LISA P-value calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate p-values, mean, and variance\n",
    "def calculate_statistics(observed_Is, simulated_Is):\n",
    "    num_simulations = simulated_Is.shape[0]\n",
    "    # Calculate p-values for positive and negative spatial autocorrelation\n",
    "    p_values_positive = np.sum(simulated_Is >= observed_Is, axis=0) / num_simulations\n",
    "    p_values_negative = np.sum(simulated_Is <= observed_Is, axis=0) / num_simulations\n",
    "    # Combine p-values for two-sided test\n",
    "    p_values = np.minimum(p_values_positive, p_values_negative) * 2\n",
    "    empirical_means = np.mean(simulated_Is, axis=0)\n",
    "    empirical_vars = np.var(simulated_Is, axis=0)\n",
    "    z_scores = (observed_Is - empirical_means) / np.sqrt(empirical_vars)\n",
    "\n",
    "    return p_values, empirical_means, empirical_vars, z_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting saved simulated LISAs\n",
    "simulated_LISA_df = pd.read_csv('results/simulated_LISAs_STAC.csv')\n",
    "# Changing column names to resemble LISA values\n",
    "regions = list(final_df['NUTS_ID'].unique())\n",
    "simulated_LISA_df.columns = regions\n",
    "# Converting data types of input variables for statistics calculation function\n",
    "# Convert dictionary values to a numpy array\n",
    "observed_Is = np.array(list(local_STAC_dict.values()))\n",
    "simulated_Is = simulated_LISA_df.values\n",
    "# Calling the statistic function\n",
    "# Example usage of calculate_statistics\n",
    "p_values, empirical_means, empirical_vars, z_scores = calculate_statistics(observed_Is, simulated_Is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Adding p-values to final LISA_df\n",
    "# Creating a copy to prevent confusion when slicing\n",
    "region_df_copy = region_df.copy()\n",
    "# Create a DataFrame with the extracted p-values\n",
    "p_values_df = pd.DataFrame(p_values, columns=['LISA_p_values'])\n",
    "region_df_copy['LISA_p_values'] = p_values_df['LISA_p_values']\n",
    "# Adding a significance column to indicate LISAs below 0.05\n",
    "region_df_copy['LISA_significant'] = region_df_copy['LISA_p_values'].apply(lambda x: 'Significant' if x < 0.05 else 'Non-Significant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the new updated LISA_df since it now has significant LISA regions\n",
    "region_df_copy.to_csv(\"results/STAC_LISA_df.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}